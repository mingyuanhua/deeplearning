{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPywt/bGMYpLBotu435v/Hc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug6iLYgwQBjP","executionInfo":{"status":"ok","timestamp":1745121459899,"user_tz":420,"elapsed":827,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"fa980119-8c67-43ab-9410-fd9f7bfa63bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# unzip images\n","# !unzip /content/drive/MyDrive/data/ClassifyLeaves/classify-leaves.zip -d /content/drive/MyDrive/data/ClassifyLeaves"],"metadata":{"id":"Oil_ENGIMj8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","path = '/content/drive/MyDrive'\n","os.chdir(path)\n","\n","!source venv_d2l/bin/activate"],"metadata":{"id":"2HTjPc3YO-yA","executionInfo":{"status":"ok","timestamp":1745121461982,"user_tz":420,"elapsed":146,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/d2l-ai/d2l-en.git"],"metadata":{"id":"EftcoFUoQGtd","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# two additional libraries that you will need to install for this notebook if you run it on kaggle or colab\n","!pip install timm\n","!pip install madgrad"],"metadata":{"id":"CcT0-KenQI2z","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1745119274051,"user_tz":420,"elapsed":144746,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"1443e82b-cdaa-426d-b470-050e831f34cc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.23.5)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting madgrad\n","  Downloading madgrad-1.3.tar.gz (7.9 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: madgrad\n","  Building wheel for madgrad (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for madgrad: filename=madgrad-1.3-py3-none-any.whl size=11949 sha256=9d29bd799069012b310503ef25003cb44ae18a4f9f8ce85428879e2cb66c3a44\n","  Stored in directory: /root/.cache/pip/wheels/9d/d8/f7/f29e91274f40f339d572cc917a8a6e5307fc0167c682f45655\n","Successfully built madgrad\n","Installing collected packages: madgrad\n","Successfully installed madgrad-1.3\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/d2l-zh'\n","os.chdir(path)"],"metadata":{"id":"O0GC5blGQK3Q","executionInfo":{"status":"ok","timestamp":1745121464510,"user_tz":420,"elapsed":6,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import pandas as pd\n","# import numpy as np\n","# from torch.utils.data import Dataset, DataLoader\n","# from torchvision import transforms\n","# from PIL import Image\n","# import os\n","# import matplotlib.pyplot as plt\n","# import torchvision.models as models\n","# # This is for the progress bar.\n","# from tqdm import tqdm\n","# import seaborn as sns\n","\n","# ====================================================\n","# Library\n","# ====================================================\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam, SGD\n","from torchvision import datasets, models, transforms\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import cv2\n","import timm\n","\n","import time\n","import os\n","import copy\n","from tqdm import tqdm\n","import random\n","from madgrad import MADGRAD\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"BqbIxrsFQMqV","executionInfo":{"status":"ok","timestamp":1745121497888,"user_tz":420,"elapsed":30921,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","train = pd.read_csv('/content/drive/MyDrive/data/ClassifyLeaves/train.csv')\n","\n","def get_image_file_path(image_path):\n","    INPUT_DIR = '/content/drive/MyDrive/data/ClassifyLeaves/images'\n","    return INPUT_DIR+image_path\n","\n","# ====================================================\n","# Leave labels mapping\n","# ====================================================\n","species_name_list = sorted(set(train['label']))\n","species_to_num = dict(zip(species_name_list, range(len(species_name_list))))\n","num_to_species = {value : key for key, value in species_to_num.items()}\n","num_class = len(species_name_list)"],"metadata":{"id":"5M-BhxEnWh3R","executionInfo":{"status":"ok","timestamp":1745121731010,"user_tz":420,"elapsed":1187,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    debug=False\n","    print_freq=100\n","    num_workers=4\n","    model_name='inception_resnet_v2'\n","    size=299\n","    epochs=50 # not to exceed 9h\n","    batch_size=32\n","    learning_rate=1e-4\n","    weight_decay=1e-9\n","    scheduler='ReduceLROnPlateau' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n","    factor=0.2 # ReduceLROnPlateau\n","    patience=4 # ReduceLROnPlateau\n","    eps=1e-6 # ReduceLROnPlateau\n","    T_max=20 # CosineAnnealingLR\n","    #T_0=4 # CosineAnnealingWarmRestarts\n","    min_lr=1e-6 # ['CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n","    seed=42 #[42,2021]\n","    n_fold=5\n","    train=True"],"metadata":{"id":"Is7mTTmUQ1It","executionInfo":{"status":"ok","timestamp":1745121767580,"user_tz":420,"elapsed":9,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# Utils\n","# ====================================================\n","def init_logger(log_file='./train.log'):\n","    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()\n","\n","def seed_torch(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_torch(seed=CFG.seed)"],"metadata":{"id":"6GSgOX-qQ-pi","executionInfo":{"status":"ok","timestamp":1745121781112,"user_tz":420,"elapsed":38,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# CV split\n","# ====================================================\n","folds = train.copy()\n","Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n","    folds.loc[val_index, 'fold'] = int(n)\n","folds['fold'] = folds['fold'].astype(int)\n","print(folds.groupby(['fold']).size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxQXiRojQ-ch","executionInfo":{"status":"ok","timestamp":1745121797364,"user_tz":420,"elapsed":324,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"79d5bd79-955b-4f6c-b77b-f377cd2ad2fe"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["fold\n","0    3671\n","1    3671\n","2    3671\n","3    3670\n","4    3670\n","dtype: int64\n"]}]},{"cell_type":"code","source":["# ====================================================\n","# Dataset\n","# ====================================================\n","# note: here I made a small mistake. I did not notice the cv2.imread generate BGR image until the last week when I went through other's codes, so most of my models were trained using BGR image\n","# I think using RGB image may make the training converge faster when we start from imagenet pre-trained model\n","class TrainDataset(Dataset):\n","    def __init__(self, df, species_to_num, transform=None):\n","        super().__init__()\n","        self.df = df\n","        self.species_to_num = species_to_num\n","        self.file_paths = get_image_file_path(df['image'].values)\n","        self.labels = df['label'].values\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        image = cv2.imread(file_path)\n","#         image = Image.open(file_path)\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        label = self.species_to_num[label]\n","        return image, label\n","\n","\n","class TestDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        super().__init__()\n","        self.df = df\n","        self.file_paths = get_image_file_path(df['image'].values)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        image = cv2.imread(file_path)\n","#         image = Image.open(file_path)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image"],"metadata":{"id":"zpCR2ihrRGRE","executionInfo":{"status":"ok","timestamp":1745121844296,"user_tz":420,"elapsed":2,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# Data transforms\n","# ====================================================\n","# I applied slightly different transforms for different groups of models\n","# resnet50d, efficientnet_b3: flip only\n","# resnext50_32x4d, resnest50d tf_efficientnet_b4_ns, resnest200e, mixnet_s: add ColorJitter\n","# inception_resnet_v2, vit_base_patch16_224, tf_efficientnet_b3_ns: use RandomResizedCrop instead of Resize, and use [0.5, 0.5, 0.5] as mean and std\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),\n","        transforms.RandomResizedCrop([CFG.size, CFG.size]),\n","#         transforms.Resize([CFG.size, CFG.size]),\n","        transforms.Normalize(\n","            mean=[0.5, 0.5, 0.5],\n","            std=[0.5, 0.5, 0.5],\n","#             mean=[0.485, 0.456, 0.406],\n","#             std=[0.229, 0.224, 0.225],\n","        ),\n","        #transforms.RandomErasing(),\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize([CFG.size, CFG.size]),\n","        transforms.Normalize(\n","            mean=[0.5, 0.5, 0.5],\n","            std=[0.5, 0.5, 0.5],\n","#             mean=[0.485, 0.456, 0.406],\n","#             std=[0.229, 0.224, 0.225],\n","        ),\n","    ]),\n","}"],"metadata":{"id":"N3s7cCk7RWsB","executionInfo":{"status":"ok","timestamp":1745121861079,"user_tz":420,"elapsed":28,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# scheduler\n","# ====================================================\n","def get_scheduler(optimizer):\n","    if CFG.scheduler=='ReduceLROnPlateau':\n","        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n","    elif CFG.scheduler=='CosineAnnealingLR':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n","    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n","    return scheduler"],"metadata":{"id":"x9-g0Pd7RWfA","executionInfo":{"status":"ok","timestamp":1745121878292,"user_tz":420,"elapsed":42,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# training\n","# ====================================================\n","def train_model(fold):\n","    since = time.time()\n","    model_path = './models/' + CFG.model_name + '_fold' + str(fold) + '_best.pth'\n","    LOGGER.info(f\"============================== fold: {fold} result ==============================\")\n","    # ====================================================\n","    # Data Loader\n","    # ====================================================\n","    trn_idx = folds[folds['fold'] != fold].index\n","    val_idx = folds[folds['fold'] == fold].index\n","    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n","    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n","    valid_labels = valid_folds['label'].values\n","    train_dataset = TrainDataset(train_folds, species_to_num, transform=data_transforms['train'])\n","    valid_dataset = TrainDataset(valid_folds, species_to_num, transform=data_transforms['valid'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size,\n","                              shuffle=True, num_workers=CFG.num_workers,\n","                              pin_memory=True, drop_last=True,)\n","    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size,\n","                              shuffle=False, num_workers=CFG.num_workers,\n","                              pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model, optimizer, scheduler & loss function\n","    # ====================================================\n","    model = timm.create_model(CFG.model_name, pretrained=True, num_classes=num_class)\n","    # use the following model function if you want to fine-tune the last layer only, which is not what I did here. But I explored it during the competition.\n","    #model = leave_classifier(CFG.model_name, num_class)\n","    model = model.to(device)\n","\n","    #optimizer = Adam(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay, amsgrad=False)\n","    optimizer = MADGRAD(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n","    scheduler = get_scheduler(optimizer)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","\n","    best_acc = 0.95 # do not save the model if the acc is less than 0.95\n","\n","    for epoch in range(CFG.epochs):\n","\n","        # ---------- Training ----------\n","        # Make sure the model is in train mode before training.\n","        model.train()\n","        # These are used to record information in training.\n","        train_loss = []\n","        train_accs = []\n","\n","        global_step = 0\n","        # Iterate the training set by batches.\n","        for step, (imgs, labels) in enumerate(train_loader):\n","            # A batch consists of image data and corresponding labels.\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","            # Forward the data. (Make sure data and model are on the same device.)\n","            logits = model(imgs)\n","            # Calculate the cross-entropy loss.\n","            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n","            loss = criterion(logits, labels)\n","\n","            # Gradients stored in the parameters in the previous step should be cleared out first.\n","            optimizer.zero_grad()\n","            # Compute the gradients for parameters.\n","            loss.backward()\n","            # Update the parameters with computed gradients.\n","            optimizer.step()\n","\n","            # Compute the accuracy for current batch.\n","            acc = (logits.argmax(dim=-1) == labels).float().mean()\n","\n","            # Record the loss and accuracy.\n","            train_loss.append(loss.item())\n","            train_accs.append(acc)\n","\n","        # The average loss and accuracy of the training set is the average of the recorded values.\n","        train_loss = sum(train_loss) / len(train_loss)\n","        train_acc = sum(train_accs) / len(train_accs)\n","\n","\n","        # ---------- Validation ----------\n","        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n","        model.eval()\n","        # These are used to record information in validation.\n","        valid_loss = []\n","        valid_accs = []\n","\n","        # Iterate the validation set by batches.\n","        for step, (imgs, labels) in enumerate(valid_loader):\n","            # We don't need gradient in validation.\n","            # Using torch.no_grad() accelerates the forward process.\n","            with torch.no_grad():\n","                logits = model(imgs.to(device))\n","\n","            # We can still compute the loss (but not the gradient).\n","            loss = criterion(logits, labels.to(device))\n","\n","            # Compute the accuracy for current batch.\n","            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","            # Record the loss and accuracy.\n","            valid_loss.append(loss.item())\n","            valid_accs.append(acc)\n","\n","        # The average loss and accuracy for entire validation set is the average of the recorded values.\n","        valid_loss = sum(valid_loss) / len(valid_loss)\n","        valid_acc = sum(valid_accs) / len(valid_accs)\n","\n","\n","        # learning rate update\n","        if isinstance(scheduler, ReduceLROnPlateau):\n","            scheduler.step(valid_acc)\n","        else:\n","            scheduler.step()\n","\n","        elapsed = time.time() - since\n","\n","        # Print the information.\n","        LOGGER.info(f'Epoch {epoch+1}/{CFG.epochs}: train_loss = {train_loss:.4f}, valid_loss = {valid_loss:.4f}, train_acc = {train_acc:.4f}, valid_acc = {valid_acc:.4f}, time: {elapsed:.0f}s')\n","        #print(f'learning_rate = {scheduler.optimizer.param_groups[0]['lr']}')\n","        # if the model improves, save a checkpoint at this epoch\n","\n","        if valid_acc > best_acc:\n","            best_acc = valid_acc\n","            torch.save(model.state_dict(), model_path)\n","            LOGGER.info(f'Save Best Score: {best_acc:.4f} Model to {model_path}')\n","            #print('saving model with acc {:.3f}'.format(best_acc))"],"metadata":{"id":"uajWQgJaUuFB","executionInfo":{"status":"ok","timestamp":1745121921208,"user_tz":420,"elapsed":105,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# once you fix the random seeds, you can train different folds and different models on different machines, e.g., kaggle, colab, etc.\n","# set appropriate epochs and the number of folds to remote training.\n","# the models I used in this competition usually takes 1-4 min for each epoch on V100, and the time cost may double up on other GPUs like P100 and T4\n","# train_model(2)"],"metadata":{"id":"eWfC1fuTUt6x","executionInfo":{"status":"ok","timestamp":1745121987131,"user_tz":420,"elapsed":3,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Model Selection\n","# Here I used out-of-fold predictions to develop ensemble models\n","def cv_prob(fold):\n","    test_dataset = TestDataset(folds[folds['fold']==fold], transform=data_transforms['valid'])\n","    test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","    model_path = CFG.model_dir + CFG.model_name + '_fold' + str(fold) + '_best.pth'\n","    model = timm.create_model(CFG.model_name, pretrained=False, num_classes=num_class)\n","    model = model.to(device)\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","\n","    prob_list = []\n","\n","    # Iterate the testing set by batches.\n","    for batch in tqdm(test_loader):\n","        imgs = batch\n","        with torch.no_grad():\n","            logits = model(imgs.to(device))\n","            prob_list.append(logits.softmax(1))\n","    probs_np = torch.cat(prob_list, axis=0).to('cpu').numpy()\n","    return probs_np"],"metadata":{"id":"0xUasatIYEn4","executionInfo":{"status":"ok","timestamp":1745122017870,"user_tz":420,"elapsed":2,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["for fold in range(CFG.n_fold):\n","    probs_np = cv_prob(fold)\n","    folds.loc[(folds['fold']==fold),CFG.model_name] = np.argmax(probs_np,axis=1)\n","    if(fold==0):\n","        probs_np_copy = probs_np\n","    else:\n","        probs_np_copy = np.concatenate((probs_np_copy, probs_np), axis=0)\n","    print(probs_np_copy.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"aZueVW0AYMH_","executionInfo":{"status":"error","timestamp":1745122023930,"user_tz":420,"elapsed":164,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"119f8e31-3aca-41be-8495-df8101533f86"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"AttributeError","evalue":"type object 'CFG' has no attribute 'model_dir'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-309458dfb16e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprobs_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprobs_np_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-0caf1f674a6a>\u001b[0m in \u001b[0;36mcv_prob\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_fold'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_best.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: type object 'CFG' has no attribute 'model_dir'"]}]},{"cell_type":"code","source":["# after I collected all fine-tuned models, I created a 3D array probs_3D = np.zeros([train.shape[0],num_class,num_models]) to save all softmax outputs for all the models\n","# probs_3D = np.zeros([train.shape[0],num_class,num_models])\n","# probs_3D[:,:,2] = probs_np_copy"],"metadata":{"id":"0pLBbB-KYNkR","executionInfo":{"status":"ok","timestamp":1745122047141,"user_tz":420,"elapsed":37,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# convert the species name to the index for easier comparison\n","folds['label_num'] = folds['label'].map(species_to_num)"],"metadata":{"id":"fbhWVojFYTQ7","executionInfo":{"status":"ok","timestamp":1745122054023,"user_tz":420,"elapsed":3,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Stacked mean combinations\n","from itertools import combinations, chain\n","combined = []\n","num_model = probs_3D.shape[2]\n","for i in range(num_model):\n","    combined.append(list(combinations(range(num_model), i+1)))\n","# sort the folds to match the index used in probs_3D, the softmax output\n","fold_sorted = folds.rename_axis('MyIdx').sort_values(by = ['fold', 'MyIdx'], ascending = [True, True])\n","\n","comb_results = dict()\n","with tqdm(total=len(list(chain(*combined)))) as process_bar:\n","    for c in list(chain(*combined)):\n","        # the result indicates how many out-of-fold predictions are incorrect\n","        comb_results[c] = (fold_sorted['label_num']!=np.argmax(probs_3D[:,:,c].sum(2),axis=1)).sum()"],"metadata":{"id":"p_W8lz-RYU9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["{k: comb_results[k] for k in sorted(comb_results, key=comb_results.get, reverse=False)[0:20]}"],"metadata":{"id":"ZmVzY8ySYkVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Weighted average\n","num_model = probs_3D.shape[2]\n","weights = np.array([1.0/n_models for _ in range(num_model)])\n","bounds = [(0.0, 1.0) for _ in range(num_model)]"],"metadata":{"id":"18OavXonYmDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_func(w):\n","    # use 1 - accuracy as the loss function to find weights\n","    w= np.ceil(np.array(w)*20) # this operation is to lower the resolution of the weights\n","    return (fold_sorted['label_num']!=np.argmax(np.matmul(probs_3D,w).reshape(-1, num_class),axis=1)).sum()\n"],"metadata":{"id":"lcA1vdp9YprU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.optimize import differential_evolution\n","sol = differential_evolution(loss_func, bounds, maxiter=20, tol=1e-4, disp=True)\n","# sol.x is the final weight vector\n","# In fact, I did not use weighted average for my submission. I used it to check the importance of each model and confirmed that stacked mean method is good enough"],"metadata":{"id":"Yhg9_Ad1Yr8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inference\n","test = pd.read_csv('../input/test.csv')"],"metadata":{"id":"6boNMJw3Ytiv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = TestDataset(test, transform=data_transforms['valid'])\n","test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False,\n","                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"],"metadata":{"id":"AskR-oClYwxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_prob():\n","    model = timm.create_model(CFG.model_name, pretrained=False, num_classes=num_class)\n","    model = model.to(device)\n","    avg_preds = []\n","    for fold in range(CFG.n_fold):\n","        model_path = CFG.model_dir + CFG.model_name + '_fold' + str(fold) + '_best.pth'\n","        model.load_state_dict(torch.load(model_path))\n","        model.eval()\n","        prob_list = []\n","        for batch in tqdm(test_loader):\n","            imgs = batch\n","            with torch.no_grad():\n","                logits = model(imgs.to(device))\n","            prob_list.append(logits.softmax(1)) #(batch_size x num_class)\n","        probs_np = torch.cat(prob_list, axis=0).to('cpu').numpy()\n","        avg_preds.append(probs_np) #(test_size x num_class)\n","    probs = np.mean(avg_preds, axis=0) # average of 5-fold prediction\n","    return probs"],"metadata":{"id":"mHXE235yYyer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I implement the method with TTA, but I did not submit any predictions with TTA\n","# def test_prob_with_TTA():\n","#     model = timm.create_model(CFG.model_name, pretrained=False, num_classes=num_class)\n","#     model = model.to(device)\n","#     avg_preds = []\n","#     for fold in range(CFG.n_fold):\n","#         model_path = CFG.model_dir + CFG.model_name + '_fold' + str(fold) + '_best.pth'\n","#         model.load_state_dict(torch.load(model_path))\n","#         model.eval()\n","#         prob_list = []\n","#         for batch in tqdm(test_loader):\n","#             x = batch.to(device)\n","#             with torch.no_grad():\n","#                 x = torch.stack([x, x.flip(-1), x.flip(-2), x.flip(-1,-2),\n","#                                  x.transpose(-1,-2), x.transpose(-1,-2).flip(-1),\n","#                                  x.transpose(-1,-2).flip(-2), x.transpose(-1,-2).flip(-1,-2)],0)\n","#                 x = x.view(-1, 3, CFG.size, CFG.size)\n","#                 logits = model(x)\n","#                 logits = logits.view(8, CFG.batch_size, -1).mean(0)\n","#             prob_list.append(logits.softmax(1)) #(batch_size x num_class)\n","#         probs_np = torch.cat(prob_list, axis=0).to('cpu').numpy()\n","#         avg_preds.append(probs_np) #(test_size x num_class)\n","#     probs = np.mean(avg_preds, axis=0) # average of 5-fold prediction\n","#     return probs"],"metadata":{"id":"ZOwBnXRfY0pw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# just like what I did in model selection section, I create a 3D array to save all softmax outputs\n","probs_3D_pred = np.zeros([8800,176,11])"],"metadata":{"id":"pkyXpW7IY20R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs_np_copy2 = test_prob()\n","probs_np_copy2.shape"],"metadata":{"id":"qsSKveMcY4lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# once the softmax outputs are saved, I do not have to run the prediction again for this model, although it only takes a few minites.\n","probs_3D_pred[:,:,0] = probs_np_copy2\n","# you only need to do ensemble after doing this for all 11 models\n","np.save('prediction_raw_data_11models.npy',probs_3D_pred)"],"metadata":{"id":"1dNX2iLdY58U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uploading the models is too time-consuming. Instead, I uploaded the softmax output.\n","\n","import numpy as np\n","import pandas as pd\n","probs_3D_ori_11 = np.load('../input/model-predictions/prediction_raw_data_11models.npy')\n","test = pd.read_csv('../input/classify-leaves/test.csv')\n","train = pd.read_csv('../input/classify-leaves/train.csv')\n","\n","# ====================================================\n","# Leave labels mapping\n","# ====================================================\n","species_name_list = sorted(set(train['label']))\n","species_to_num = dict(zip(species_name_list, range(len(species_name_list))))\n","num_to_species = {value : key for key, value in species_to_num.items()}\n","num_class = len(species_name_list)"],"metadata":{"id":"4xuVdPJhY7aw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s6_pred = pd.Series(np.argmax(probs_3D_ori_11[:,:,(0, 1, 3, 4, 6, 7, 9)].sum(2),axis=1))"],"metadata":{"id":"cc4PoXimZDsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test['label'] = s6_pred.map(num_to_species)\n","submission = pd.concat([test['image'], test['label']], axis=1)\n","submission.head()"],"metadata":{"id":"b-prYcvQZGs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('./submission_6.csv', index=False)\n","# !kaggle competitions submit -c classify-leaves -f submission_6.csv -m \"0, 1, 3, 4, 6, 7, 9 CV98.33\""],"metadata":{"id":"u3AHNwrnZIDn"},"execution_count":null,"outputs":[]}]}