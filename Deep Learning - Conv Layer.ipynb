{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdOZjDJoeSf585yDRI4bsZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8cj4sb20CQl","executionInfo":{"status":"ok","timestamp":1712766131900,"user_tz":420,"elapsed":24990,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"66ba24ad-f515-4d01-d96b-01980881eee9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install d2l==0.17.6"],"metadata":{"id":"m5S2bQVg0L1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","path = '/content/drive/MyDrive'\n","os.chdir(path)\n","\n","!source venv_d2l/bin/activate\n","\n","path = '/content/drive/MyDrive/d2l-zh'\n","os.chdir(path)"],"metadata":{"id":"7Tn-Wwkc0Qxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["分类猫和狗的图片\n","- 使用还不错的相机采集图片（1200万像素）\n","- RGB图片有3600万元素 Input layer 特征\n","- 使用100大小的单隐藏层MLP，模型有36亿参数=14GB\n","- 远多于世界上所有猫和狗的总数（9亿狗，6亿猫）\n","- 单层就要14GB的内存，还不算做运算\n","\n","Waldo 两个原则 在图片里找waldo的原则启发了我们CNN的设计\n","- 平移不变性\n","- 局部性\n","\n","卷积CNN 特殊的全连接层\n","- 全连接层，之前的图片是个矩阵，但我们是把他作为一维的向量，现在我们还是还原为矩阵，因为我们要考虑空间的一些信息，必须用矩阵来存\n","- 将输入和输出从向量变形为矩阵（宽度，高度）\n","- 将权重变形为4-D张量(h,w)到(h',w') 输入的高宽到输出的高宽\n","- V是W的重新索引\n","- 原则#1 - 平移不变性 x的平移导致h的平移 而V不应该依赖于i,j\n","- 解决方案：不管ij怎么变，v应该不变，v_i,j,a,b = v_a,b\n","- 不管ij挪到哪里，我要看的识别器v是不发生变化的，这个就叫做2维卷积，数学上来说叫2维交叉相关：输出的像素等于输入ij对应的那个像素，以他为中心不断做offset的加一点减一点往边上挪的时候，和他V的这一个模是做内积\n","- 平移不变性使得我们把ij这个维度干掉了，只剩ab这个维度\n","- 2维卷积就是我的全连接，矩阵乘法，但是我的权重使得他的一些东西是重复的，他不是每一个元素都可以自由变换。当我把一个模型的取值范围做限制的话，我就把模型复杂度降低了，同样的话，就意味着我不用存那么多东西了。\n","- 原则#2 - 局部性\n","- 假设我要去算ij的这个输出的话，我会取看以ij为中心的所有地方，但实际上来说，我们不应该去看那么多地方，ij的结果只应该由Xij附近的那些点就行了。\n","- 当评估hij时，我们不应该用远离xij的参数\n","- 解决方案：当|a|,|b| > delta时，使得v_a,b = 0. ij的点远离我超过delta的时候，那些地方就不再看了\n","- 总结：对全连接层使用平移不变性和局部性得到卷积层\n","\n","刚刚是卷积的操作子，现在来说卷积层是什么。\n","- 二维交叉相关：input, kernel 卷积核，output\n","- 二维卷积层\n","- 输入X: n_h x n_w\n","- 核W: k_h x k_w\n","- 偏差b 是一个实数\n","- 输出Y: (n_h - k_h + 1) x (n_w - k_w + 1) 平移到后面没了，丢掉的就是k_h - 1和k_w - 1\n","- Y = X * W + b\n","- W和b是可学习的参数，神经网络来学习出一些核\n","- 交叉相关和卷积的唯一区别是W_a,b和W_-a,-b, 有个负号。由于对称性，在实际使用中没有区别，因为w是我学的，其实就是上下反一下，左右反一下，实际上没区别。神经网络因为写起来方便，所以没有用数学上的卷积写法，其实是倒过来的。说是说卷积层，实际上实现的交叉相关\n","- CNN来做文本效果很好\n","- 总结：卷积层就是将输入和核矩阵进行交叉相关运算，加上偏移后得到输出。核矩阵和偏移是可学习的参数。核矩阵的大小是超参数。核参数是个很小的东西，这就是解决了随着输入的变大，参数不会变得特别大，卷积不会有这个问题。\n","\n"],"metadata":{"id":"cDyo4BsO0XFf"}},{"cell_type":"code","source":["# 图像卷积\n","# 互相关运算 cross-correlation\n","import torch\n","from torch import nn\n","from d2l import torch as d2l\n","\n","# X是输入，K是核矩阵\n","def corr2d(X, K):\n","    \"\"\"计算二维互相关运算\"\"\"\n","    h, w = K.shape\n","    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n","    for i in range(Y.shape[0]):\n","        for j in range(Y.shape[1]):\n","            # 小方块拿出来跟核矩阵做点积然后求和\n","            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n","    return Y"],"metadata":{"id":"zg8F2gB80Qu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 验证上述二维互相关运算的输出\n","X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n","K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n","corr2d(X, K)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hp5I8-VZe50","executionInfo":{"status":"ok","timestamp":1712775985851,"user_tz":420,"elapsed":203,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"a1ea6e61-6e95-4679-cf17-e25bfe1f2e8a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[19., 25.],\n","        [37., 43.]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# 实现二维卷积层\n","# 用继承nn.Module来实现一个类\n","class Conv2D(nn.Module):\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.rand(kernel_size)) # kernel_size超参数\n","        self.bias = nn.Parameter(torch.zeros(1)) # 标量\n","\n","    # 前向运算\n","    def forward(self, x):\n","        x和weight做互相关运算\n","        return corr2d(x, self.weight) + self.bias"],"metadata":{"id":"JmzoFLrl0QsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 卷积层的一个简单应用：检测图像中不同颜色的边缘\n","X = torch.ones((6, 8))\n","X[:, 2:6] = 0\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVrn5ChZ0Qpe","executionInfo":{"status":"ok","timestamp":1712779956077,"user_tz":420,"elapsed":199,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"8a588c8b-3ef0-435d-f53f-3f13c460a795"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n","        [1., 1., 0., 0., 0., 0., 1., 1.],\n","        [1., 1., 0., 0., 0., 0., 1., 1.],\n","        [1., 1., 0., 0., 0., 0., 1., 1.],\n","        [1., 1., 0., 0., 0., 0., 1., 1.],\n","        [1., 1., 0., 0., 0., 0., 1., 1.]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 假设两个元素没有边的话，输出就是0，如果1变0或0变1，输出要么1要么-1\n","K = torch.tensor([[1.0, -1.0]])\n","Y = corr2d(X, K)\n","Y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GL1JjK5Lo3iu","executionInfo":{"status":"ok","timestamp":1712780058382,"user_tz":420,"elapsed":260,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"4db08d22-5445-437b-f61f-47882457de51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n","        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n","        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n","        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n","        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n","        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 卷积核K只可以检测垂直边缘\n","# 转置后就不能检测东西了\n","corr2d(X.t(), K)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QoSEf88pdLB","executionInfo":{"status":"ok","timestamp":1712780125817,"user_tz":420,"elapsed":193,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"b8cdfed6-3a30-4035-b30d-b77f2348ecdc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 学习由X生成Y的卷积核\n","# 现在是给定X和Y，来学这个K\n","\n","# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n","# 调用了nn的Conv2d函数，第一个1是输入的通道，单个矩阵，黑白图片通道为1，彩色图片通道为3\n","# 第二个1是输出通道，现在也为1；核我们知道是个1x2的核，来学他；不需要bias\n","conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n","\n","# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n","# 其中批量大小和通道数都为1\n","X = X.reshape((1, 1, 6, 8))\n","Y = Y.reshape((1, 1, 6, 7))\n","lr = 3e-2  # 学习率\n","\n","# 手写了一个训练模型 迭代10轮\n","# 每一次把X放到conv2d里得到Y_hat，然后计算均方误差作为loss\n","# 然后把conv2d的梯度设为0，loss求和后算个backward，这儿是梯度下降\n","# conv2d.weight.data[:] 做inplace操作，-=学习率*梯度\n","# 每两个batch后print loss\n","for i in range(10):\n","    Y_hat = conv2d(X)\n","    l = (Y_hat - Y) ** 2\n","    conv2d.zero_grad()\n","    l.sum().backward()\n","    # 迭代卷积核\n","    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n","    if (i + 1) % 2 == 0:\n","        print(f'epoch {i+1}, loss {l.sum():.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyrfqPQK0Qmw","executionInfo":{"status":"ok","timestamp":1712773499187,"user_tz":420,"elapsed":244,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"6a19caef-4a29-415a-f2f7-92c110851232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 2, loss 4.095\n","epoch 4, loss 0.799\n","epoch 6, loss 0.180\n","epoch 8, loss 0.049\n","epoch 10, loss 0.016\n"]}]},{"cell_type":"code","source":["# 所学的卷积核的权重向量\n","# 跟我们自己构造出来的(1,-1)很接近了\n","conv2d.weight.data.reshape((1, 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpftGnoo0Qj-","executionInfo":{"status":"ok","timestamp":1712773521414,"user_tz":420,"elapsed":310,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"9167b1bd-3d58-4649-ea67-1d0d7ae554b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.0019, -0.9783]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["为什么kernel不是越大越好？\n","- 最终是想看到所有的没错。类似于问为什么全连接层为什么不是隐藏层越大越好。可以做一个很浅很宽的全连接层，但很多时候没有一个深一点的窄一点的全连接层效果好。所以神经网络也是一样。\n","- 我做一个卷积层层数比较少的1-2层，但是每个核比较大的，不然每一层把核变小一点，然后把他做深一点。最后的视野是一样的，每次看一点看一点。所以核主流用3x3，最多用5x5.\n","\n","\n","房价竞赛时，损失随迭代次数变化图抖动很厉害：\n","- 可能是数据多样性比较大，这个抖动是没关系的，只要下降\n","- 可以把批量大小增大或者学习率给高一点\n","\n","全连接层最大的问题是权重W的高是取决于输入的宽，当给一个1200万像素的图片的时候，输入的维度就变成1200万的维度了。\n","- 所以用卷积就没这个问题，不管输入多大，核的大小是固定的。\n","- 实际应用中，我们不会把1200万丢进去，而是200x200的图片，但是全连接层不是那么好用。"],"metadata":{"id":"uc2VpEJbsQJC"}}]}