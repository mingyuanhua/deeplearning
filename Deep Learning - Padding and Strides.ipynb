{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25903,"status":"ok","timestamp":1714588457623,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"},"user_tz":420},"id":"iKjNlbBVF1q0","outputId":"7a351b18-3a99-4e0c-9226-4f98ce1778fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkaVRjQSF_hz"},"outputs":[],"source":["!pip install d2l==0.17.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlHaBzqnF_Xx"},"outputs":[],"source":["import os\n","path = '/content/drive/MyDrive'\n","os.chdir(path)\n","\n","!source venv_d2l/bin/activate\n","\n","path = '/content/drive/MyDrive/d2l-zh'\n","os.chdir(path)"]},{"cell_type":"markdown","source":["卷积层里的填充和步幅\n","- 卷积层控制输出大小的两个超参数：一个叫填充，一个叫步幅。\n","- 为什么我们需要填充？假设给定一个32x32的图像，应用5x5大小的卷积核，第一层得到输出大小28x28，第7层得到输出大小4x4，更大的卷积核可以更快地减小输出大小，形状从n_h x n_w减少到(n_h - k_h + 1) x (n_w - k_w + 1)。会有个问题，假设不想输出变得这么小，或者说我们想用很多层的时候，因为现在的输出太小了，就无法再用了。那么如果我想用更深的神经网络怎么办？因为我们整个深度学习就是希望用更深的模型，例如几百层的模型。\n","\n","填充 padding\n","- 在输入周围添加额外的行/列\n","- 填充p_h行和p_w列，输出形状为 (n_h - k_h + p_h + 1) x (n_w - k_w + p_w + 1)。\n","- 通常取p_h = k_h - 1, p_w = k_w - 1。通常会填充核的高宽减一的值。好处是输入输出形状不会发生变化。\n","- 当k_h为奇数：在上下两侧填充p_h/2；\n","- 当k_h为偶数：在上侧填充p_h/2向上取整，在下侧填充p_h/2向下取整。很少会用偶数的卷积核。\n","\n","\n","步幅 stride\n","- 填充减小的输出大小与层数线性相关\n","- 给定输入大小224x224，在使用5x5卷积核的情况下，需要44层将输出降低到4x4\n","- 需要大量计算才能得到较小输出\n","- 比较深的神经网络，他通常的输入都是比较大的224x224，实际上是个很小的图片，这样44层就比较痛苦，虽然可以使用较大的卷积核，但是我们通常用5x5或3x3，那么就需要大量的计算才能得到较小的输出。\n","- 步幅是来解决这个问题。输出的大小是跟层数线性相关的，步幅可以让他变成指数相关。\n","- 之前移动窗口的时候都是往右或往下移一格，步幅就是每次可以不要移一格，而是移两格。\n","- 步幅是指行/列的滑动步长。例：高度3宽度2的步幅。\n","- 给定高度s_h和宽度s_w的步幅，输出形状是 (n_h - k_h + p_h + s_h)/s_h 向下取整 X (n_w - k_w + p_w + s_w)/s_w 向下取整。\n","- 如果 p_h = k_h - 1, p_w = k_w - 1, 那么(n_h + s_h - 1)/s_h 向下取整 X (n_w + s_w - 1)/s_w 向下取整。\n","- 如果输入高度和宽度可以被步幅整除，那么 (n_h/s_h) X (n_w/s_w)。步幅通常取2，n_h和n_w取偶数。\n","\n","总结\n","- 填充和步幅是卷积层的超参数\n","- 填充在输入周围添加额外的行/列，通常为0，来控制输出形状的减少量，让输出形状不变或者变大\n","- 步幅是每次滑动核窗口时的行/列的步长，可以成倍的减少输出形状。当输入的大小比较大的时候，可以减少计算量\n","\n","- 一般来说，填充会使得输入和输出一样，填充通常是取(核-1)，这样纯粹是算起来比较方便。\n","- 通常来讲，步幅等于1是最好的，如果计算量太大，步幅通常取2，每次减半。什么时候取2看复杂度，例如224输入，需要100层，就把5个步幅是2的神经网络均匀地插在神经网络中间。\n","- 核大小通常是最关键的。一般来说，卷积核就是3x3.\n","- 一般来说不会真的手写神经网络，而是用ResNet系列。很多时候网络结构没有那么关键。取决于数据怎么做预处理等等。\n","- 为什么用3x3的卷积核？视野没问题，我需要用比较深的神经网络，使得最后一层每一个元素能看到足够多的图片信息。\n","- NAS可以让超参数一起训练。\n","- 从信息论的角度来看信息总是会丢失的。机器学习本质上是压缩，所有机器学习算法永远是会丢失信息的。\n","- 验证集选的比较好的话，可以很好的控制过拟合。\n","- 虽然10层3x3等价于5-6层5x5的卷积，但是3x3训练会快一些，因为计算复杂度跟高宽相关。所以用小的卷积会快一些，这也是GoogleNet最核心的设计思路。\n","- 主流的做法是，底层用相对大一点的kernel，例如5x5, 7x7, 甚至11x11的kernel，但是之后都用3x3。如果效果不明显就用一样的。\n","- 深度学习很流行，所以他不是有钱人的游戏。深度学习让GPU替代掉了很多人力成本和数据成本。NAS现在是有钱人的游戏。\n","\n","\n","\n","\n","\n"],"metadata":{"id":"7U7YoU9h5mj0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2a5wOXLPF_VK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714595665203,"user_tz":420,"elapsed":4086,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"9c0e15b5-d8f2-4fda-8d76-473810eccb9d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 8])"]},"metadata":{},"execution_count":3}],"source":["# 填充与步幅\n","# 在所有侧边填充1个像素\n","import torch\n","from torch import nn\n","\n","# 为了方便起见，我们定义了一个计算卷积层的函数\n","# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的位数\n","def comp_conv2d(conv2d, X):\n","    # 在维度的前面增加一个通道数核批量大小数\n","    # 这里的(1, 1)表示批量大小和通道数都是1\n","    X = X.reshape((1, 1) + X.shape)\n","    Y = conv2d(X)\n","    # 这是4维的，把前面两维拿掉\n","    return Y.reshape(Y.shape[2:])\n","\n","# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n","# 输入输出的通道数都是为1，核的大小为3，填充为1。之前讲的填充是左右两边一共填充的数字，对于框架来说，填充是指上下左右一边填充的数字。\n","conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n","X = torch.rand(size=(8, 8))\n","comp_conv2d(conv2d, X).shape\n","# 8 + 2 - 3 + 1 = 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3GBXX8oF_Su","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714596893194,"user_tz":420,"elapsed":208,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"9bac665d-b7a7-43fd-c688-b50d6b10aec3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 8])"]},"metadata":{},"execution_count":4}],"source":["# 填充不同的高度和宽度\n","conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n","comp_conv2d(conv2d, X).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPbaoQmZF_Os","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714596910038,"user_tz":420,"elapsed":318,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"96a42803-cdef-4d6d-d91e-d06017d91273"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 4])"]},"metadata":{},"execution_count":5}],"source":["# 将高度和宽度的步幅设置为2\n","conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n","comp_conv2d(conv2d, X).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63Ig0HTCHvJz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714596962976,"user_tz":420,"elapsed":220,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"4381ebef-1ed6-436f-de8d-4d59b8c79581"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":6}],"source":["# (8 + 2 - 5 + 4) / 4 = 2\n","conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n","comp_conv2d(conv2d, X).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Le8536EkHvGf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BcMyHOUHvDr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHUpUV66Icko"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TL6nVjMcIexh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBEx3RqbIeuO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2Q7T8uYIepL"},"outputs":[],"source":["# 多输入多输出通道 channels\n","import torch\n","from d2l import torch as d2l\n","\n","def corr2d_multi_in(X, K):\n","    # 先遍历"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XiFP9r2TIelx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AEWwuAYIeix"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldd3Wq5EIefl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byTwyuSsIec9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/N8+XT+kse5Jn+dchEnhx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}