{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1CifgDCQ9ifBLf5ugJfK6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["多输入多输出通道 channels\n","- 通道数是大家会仔细取设的超参数\n","\n","多个输入通道\n","- 每个通道都有一个卷积核，结果是所有卷积结果的和\n","- 这样输出是个单通道\n","\n","多个输出通道\n","- 无论有多少输入通道，到目前为止我们只用到单输出通道\n","- 我们可以有多个三维卷积核，每个核生成一个输出通道\n","- 输出里面的第i个通道，就是完整的输入和我对应的第i个核，也就是3D的核矩阵做多输入的卷积，整体上是4D\n","- 输入和输出通道没有太多的相关性\n","\n","我们为什么要干这个事情 - 多个输入和输出通道\n","- 每个输出通道可以识别特定模式\n","- 输入通道核识别并组合输入中的模式 - 加权后一加\n","\n","1x1卷积层\n","- 高和宽都是1的卷积层是一个受欢迎的选择。它不识别空间模式，只是融合通道。\n","- 等价于把输入拉成一个向量，输入通道数拉成一个feature数\n","\n","二维卷积层\n","- 计算复杂度（浮点计算数FLOP）O(ci co kh kw mh mw)\n","- ci=co=100 通道数等于100\n","- kh=kw=5 核宽高\n","- mh=mw=64 输出宽高\n","- 1GFLOPs = 10亿个浮点运算\n","- 10层，1M样本，10PFlops（CPU：0.15TF/s=18h， GPU：12TF/s=14min）\n","- 卷积的模型挺小的，所以存储挺小的，但是计算量一点都不小\n","\n","总结\n","- 输出通道数是卷积层的超参数，输入不是\n","- 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果\n","- 每个输出通道有独立的三维卷积核，所以最好卷积核是一个四维的张量\n","\n","\n","答疑\n","- 假设输入输出高宽不变的情况下，通常不会动通道数的值。\n","- 假设高宽都减半的情况下，通常我会把输出的通道数加一倍。我把空间信息压缩了，提取出来的信息在更多的通道上存下来。\n","- padding 0越多，计算性能只加了一点点，模型性能不会影响，0不会对输出产生影响。0和卷积一乘就得到0，虽然会有偏差，等价于一个常数。\n","- 不同通道的卷积核大小通常是一样的，这是因为计算上的好处。如果不一样写成两个卷积操作，现在一个就行。\n","- 偏移的作用越来越低。当数据的均值不为0的时候，偏移就等于均值的负数。因为我们会做大量均一化的操作，所以偏移实际上没太大影响。\n","- 核的参数是学出来的，不是选出来的。\n","- 这一节介绍的是二维卷积，如果是3d的用3维卷积，例如深度图。RGB就是通道数了，还是2维。\n","- 1x1卷积核每个输出的元素只看了对应的那个输入的像素，没有看任何输入的像素周围边上这一圈东西，只看了自己，根本没有看这个像素跟边上像素的关系是什么样子。这就是1x1卷积层不识别空间信息的由来。\n","- 卷积能有位置信息，输出第ij就是对应输入那个位置周围的信息。\n","- 每个通道学习不一样的东西，学习特定的模式，通道之间不共享参数。\n","- 所谓的feature map就是卷积的输出。\n","\n","\n","MobileNet\n","- 设计思路：depth width convolution\n","- 先对每个输入通道做3x3的卷积，不想之前把他加起来再做输出通道，再对每一个做空间融合\n","- 好处是计算量比较小，适合移动端的神经网络，计算复杂度非常低，效果现在可以到很不错的\n","\n"],"metadata":{"id":"nu-3XSqEjxMn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHYLH0V8f1zR","executionInfo":{"status":"ok","timestamp":1715288536257,"user_tz":420,"elapsed":16925,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"ff110af6-f6aa-4b15-96d3-ef71cc8b2467"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install d2l==0.17.6"],"metadata":{"id":"cb8tzKoKgExU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","path = '/content/drive/MyDrive'\n","os.chdir(path)\n","\n","!source venv_d2l/bin/activate\n","\n","path = '/content/drive/MyDrive/d2l-zh'\n","os.chdir(path)"],"metadata":{"id":"dwgqGAL_gEtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 多输入多输出通道\n","# 实现一下多输入通道互相关运算\n","\n","import torch\n","from d2l import torch as d2l\n","\n","def corr2d_multi_in(X, K):\n","    # 先遍历X和K的第0个维度（通道维度），再把它们加在一起\n","    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"],"metadata":{"id":"t0Qjntd1gErE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 验证互相关运算的输出\n","X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n","               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n","K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n","\n","corr2d_multi_in(X, K)"],"metadata":{"id":"jQ4Nafx-gEoe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715290944355,"user_tz":420,"elapsed":334,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"8d754423-2647-4921-cfc6-199be1c6e77f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 56.,  72.],\n","        [104., 120.]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 计算多个通道的输出的互相关函数\n","def corr2d_multi_in_out(X, K):\n","    # 迭代K的第0个维度，每次都对输入X执行互相关运算\n","    # 最好将所有结果都叠加在一起\n","    # K是4D的，k是3D的tensor\n","    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"],"metadata":{"id":"zCi1HMiRTqi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["K = torch.stack((K, K + 1, K + 2), 0)\n","K.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lib1r9KPTqgy","executionInfo":{"status":"ok","timestamp":1715291393630,"user_tz":420,"elapsed":156,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"eb69572e-69eb-4c1b-ecb9-1c40007244a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2, 2, 2])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["corr2d_multi_in_out(X, K)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKXGL6-5TqeX","executionInfo":{"status":"ok","timestamp":1715291475299,"user_tz":420,"elapsed":183,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"00989613-5a5c-4ed4-b4d5-6f03f4d5378e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 56.,  72.],\n","         [104., 120.]],\n","\n","        [[ 76., 100.],\n","         [148., 172.]],\n","\n","        [[ 96., 128.],\n","         [192., 224.]]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# 1x1卷积等价于一个全连接\n","def corr2d_multi_in_out_1x1(X, K):\n","    c_i, h, w = X.shape\n","    c_o = K.shape[0]\n","    # 拉成一条向量\n","    X = X.reshape((c_i, h * w))\n","    K = K.reshape((c_o, c_i))\n","    # 全连接层中的矩阵乘法\n","    # 之前是反过来写的 这里我们转置了\n","    Y = torch.matmul(K, X)\n","    return Y.reshape((c_o, h, w))"],"metadata":{"id":"-k5LYEZMTqb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = torch.normal(0, 1, (3, 3, 3))\n","K = torch.normal(0, 1, (2, 3, 1, 1))\n","\n","Y1 = corr2d_multi_in_out_1x1(X, K)\n","Y2 = corr2d_multi_in_out(X, K)\n","assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"],"metadata":{"id":"QhhWbmC6gElf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pytorch怎么调用呢？\n","# 将高度和宽度的步幅设置为2\n","conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n","comp_conv2d(conv2d, X).shape"],"metadata":{"id":"NFqdRJeXVvXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 一个稍微复杂的例子\n","conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3,4))\n","comp_conv2d(conv2d, X).shape"],"metadata":{"id":"Vm1i7zyGVvNT"},"execution_count":null,"outputs":[]}]}