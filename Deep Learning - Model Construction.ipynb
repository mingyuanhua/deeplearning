{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkoYLNEd7O6d3TPa8DMt4b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cq6iggHOFPDe","executionInfo":{"status":"ok","timestamp":1712898806055,"user_tz":420,"elapsed":20706,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"19798f9b-6785-4a4f-f986-9aa731810ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install d2l==0.17.6"],"metadata":{"id":"oJUoSnmpFUJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","path = '/content/drive/MyDrive'\n","os.chdir(path)\n","\n","!source venv_d2l/bin/activate\n","\n","path = '/content/drive/MyDrive/d2l-zh'\n","os.chdir(path)"],"metadata":{"id":"13I5Wk0iFUHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 模型构造\n","\n","\n","# 层和块\n","# 首先，我们回顾一下多层感知机\n","import torch\n","from torch import nn\n","from torch.nn import functional as F # 定义了一下没有定义参数的函数\n","\n","# 简单的单层神经网络 线性层+relu+线性层\n","net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","# nn.Sequential类定义了一个特殊的module，在pytorch里面module是一个很重要的概念\n","\n","# 2是批量大小，20是输入维度\n","# 这里相当于是多批次中，每一批的批量大小是2，特征输入维度是20\n","X = torch.rand(2, 20)\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iqmr81WFUEh","executionInfo":{"status":"ok","timestamp":1712899652053,"user_tz":420,"elapsed":5688,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"a1e339d8-e6a3-49f8-ec18-ba3653f9444a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.2056,  0.2494, -0.0433,  0.0193,  0.2222,  0.0436,  0.0318, -0.0826,\n","         -0.1385, -0.1381],\n","        [-0.0225,  0.1790,  0.0326,  0.0707,  0.3704,  0.0487, -0.0181, -0.0439,\n","         -0.1270,  0.0567]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# module可以被认为是任何一个层和神经网络都是module的一个子类\n","# 这里我们定义一个mlp 他是nn.Module的一个子类\n","# 所有的Module有两个比较重要的函数，一个是init一个是forward\n","# super().__init__()调用父类函数把初始化参数都设好\n","# 接下来定义两个全连接层\n","class MLP(nn.Module):\n","    # 用模型参数声明层。这里，我们声明两个全连接的层\n","    def __init__(self):\n","        # 调用MLP的父类Module的构造函数来执行必要的初始化\n","        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256) # 隐藏层\n","        self.out = nn.Linear(256, 10) # 输出层\n","\n","    # 定义模型的前向传播，即如何根据输入x返回所需的模型输出\n","    def forward(self, X):\n","        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义\n","        return self.out(F.relu(self.hidden(X)))"],"metadata":{"id":"xf5LjsFUFUBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层\n","net = MLP() # 实例化这个类\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8uCt9gtFT28","executionInfo":{"status":"ok","timestamp":1712899970881,"user_tz":420,"elapsed":149,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"01de789a-1864-4b2c-d38c-0f85f7b48660"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1091, -0.0735, -0.3203,  0.0730, -0.0719, -0.0036, -0.0439,  0.1774,\n","         -0.1106,  0.0956],\n","        [ 0.1648,  0.0190, -0.1834,  0.0334, -0.1987, -0.0961,  0.0122,  0.3505,\n","         -0.1035,  0.0635]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# 顺序块\n","# 这里实现了跟nn.Sequential类几乎一样的功能\n","# 定义一个nn.Module的子类，接收了一个list of input arguments\n","class MySequential(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        for idx, module in enumerate(args):\n","            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n","            # 变量_modules中。_module的类型是OrderedDict\n","            # _modules是一个特殊的容器，里面放的是我需要的层\n","            self._modules[str(idx)] = module\n","\n","    def forward(self, X):\n","        # OrderedDict保证了按照成员添加的顺序遍历它们\n","        for block in self._modules.values():\n","            X = block(X)\n","        return X"],"metadata":{"id":"bdyLmxF0ujjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDtu4jqEy4mF","executionInfo":{"status":"ok","timestamp":1712900493719,"user_tz":420,"elapsed":143,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"0fa1597f-2472-4d19-d1be-00ef44435ebd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0269,  0.2151,  0.0648,  0.1236,  0.1104,  0.2016, -0.0252, -0.0794,\n","          0.2110,  0.0042],\n","        [ 0.0653,  0.2544, -0.2109, -0.0566,  0.1574,  0.1763,  0.0491, -0.1982,\n","          0.2015, -0.0073]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 当Sequential类不能满足我们的需求的时候，我们通过自定义的能做大量计算\n","# 我们可以做很多灵活的方法来做正向传播\n","# 反向计算都是自动求导\n","\n","# 在正向传播函数中执行代码\n","class FixedHiddenMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 用torch.rand直接生成了rand_weight，这个weight是不参与训练的\n","        # requires_grad=False 不参加训练因为他不会计算梯度\n","        # 不计算梯度的随机权重参数。因此其在训练期间保存不变\n","        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n","        self.linear = nn.Linear(20, 20)\n","\n","    # forward里可以做任何事情\n","    def forward(self, X):\n","        X = self.linear(X)\n","        # 手写用rand_weight和X做矩阵乘法，+1是我们假设的偏移，再做激活函数\n","        # 然后又回过去调用一下线性类\n","        # 使用创建的常量参数以及relu和mm函数\n","        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n","        # 复用全连接层。这相当于两个全连接层共享参数\n","        X = self.linear(X)\n","        # 控制流\n","        while X.abs().sum() > 1:\n","            X /= 2\n","        return X.sum()"],"metadata":{"id":"cziAHuyry4BE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = FixedHiddenMLP()\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73vEBsJ4y39x","executionInfo":{"status":"ok","timestamp":1712900980566,"user_tz":420,"elapsed":227,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"c730a3da-0b5c-4313-c38d-dae84a377b1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.1924, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 混合搭配各种组合快的方法 -- 嵌套使用\n","class NestMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 首先定义net是一个Sequential的类和一个线性类\n","        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n","                                 nn.Linear(64, 32), nn.ReLU())\n","        self.linear = nn.Linear(32, 16)\n","\n","    def forward(self, X):\n","        return self.linear(self.net(X))\n","\n","# 对于Sequential来说输入可以是任何nn.Module的子类\n","chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n","chimera(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D47PLmEvujZh","executionInfo":{"status":"ok","timestamp":1712901135234,"user_tz":420,"elapsed":154,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"1f38a008-9ba3-4557-ab3d-b896324ff47b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.0747, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"3_wQ1NB_ujUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_TSmWRYA2u1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 参数管理\n","# 假设我们已经定义好我们的类了，参数怎么样去访问\n","# 我们首先关注具有单隐藏层的多层感知机\n","\n","import torch\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n","X = torch.rand(size=(2, 4))\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IzTQqpZ2uy7","executionInfo":{"status":"ok","timestamp":1712901338124,"user_tz":420,"elapsed":174,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"ac7cfff8-ce76-45f2-d4b2-fd3ad9519222"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.5666],\n","        [-0.7156]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 参数访问\n","# 我们要干的事情是把每一层里的权重拿出来\n","# net是nn.Sequential，Sequential可以简单的理解为python里的一个list\n","# net[2]拿到的就是nn.Linear(8, 1)最后的输出层\n","# state_dict 从自动机的角度来讲，他的权重就是他的状态\n","# 他的dict就是_modules这样的东西，是OrderedDict\n","# 这里就是最后一层的参数\n","print(net[2].state_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHuQ2vIc3zsR","executionInfo":{"status":"ok","timestamp":1712901357629,"user_tz":420,"elapsed":143,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"83d3c0fc-fe77-4d8a-ccbf-dd84b6d49155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["OrderedDict([('weight', tensor([[-0.3364, -0.1354, -0.2344, -0.0526, -0.3515, -0.0559, -0.2489, -0.0597]])), ('bias', tensor([-0.2108]))])\n"]}]},{"cell_type":"code","source":["# 目标参数\n","# Parameter定义的是一个可以优化的参数\n","# 通过.data访问真正的值，.grad访问梯度\n","print(type(net[2].bias))\n","print(net[2].bias)\n","print(net[2].bias.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGVibEfv2uwA","executionInfo":{"status":"ok","timestamp":1712901366789,"user_tz":420,"elapsed":189,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"e4357271-9ae9-4c7a-86c2-b8d1c8effc6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.nn.parameter.Parameter'>\n","Parameter containing:\n","tensor([-0.2108], requires_grad=True)\n","tensor([-0.2108])\n"]}]},{"cell_type":"code","source":["net[2].weight.grad == None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3W6-UAtU4Lsk","executionInfo":{"status":"ok","timestamp":1712901381893,"user_tz":420,"elapsed":160,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"b29379f9-bd13-4d88-e17a-93169016b66f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# 一次性访问所有参数\n","# 1就是relu，relu没有参数的，所以拿不出来\n","print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n","print(*[(name, param.shape) for name, param in net.named_parameters()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbh2FJep4Lpy","executionInfo":{"status":"ok","timestamp":1712901660482,"user_tz":420,"elapsed":166,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"07a97a34-cf74-4fb4-c9df-cde09da0dcbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n","('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"]}]},{"cell_type":"code","source":["# 访问最后一层的偏移\n","net.state_dict()['2.bias'].data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9oSGn6z5eV6","executionInfo":{"status":"ok","timestamp":1712901735054,"user_tz":420,"elapsed":148,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"19d84774-8042-47b1-a1c9-9854478ebebe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.2108])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# 从嵌套块收集参数\n","def block1():\n","    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                         nn.Linear(8, 4), nn.ReLU())\n","\n","def block2():\n","    net = nn.Sequential()\n","    for i in range(4):\n","        # add_module和之前唯一的区别是可以穿个字符串的名字，而不是直接1234了\n","        # 在这里嵌套\n","        net.add_module(f'block {i}', block1())\n","    return net\n","\n","rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n","rgnet(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cxvCPuU5rGu","executionInfo":{"status":"ok","timestamp":1712901907825,"user_tz":420,"elapsed":158,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"a557a247-93e8-45b8-ad89-8ac91c4873b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.2496],\n","        [0.2496]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 我们已经设计了网络，让我们看看它是如何组织的\n","# 三个sequential的嵌套，这里是比较简单的网络了\n","# 复杂的网络也是建议模块化\n","print(rgnet)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JPkp7FB5rTZ","executionInfo":{"status":"ok","timestamp":1712901918235,"user_tz":420,"elapsed":400,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"62453710-868e-491f-e1a9-e203e7634992"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Sequential(\n","    (block 0): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 1): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 2): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 3): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["rgnet[0][1][0].bias.data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVnO8WPA4Lme","executionInfo":{"status":"ok","timestamp":1712901935543,"user_tz":420,"elapsed":169,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"556d6506-47c1-490c-c34c-5cce6e722a3a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.2166,  0.2875,  0.3422,  0.2011,  0.2665, -0.1356,  0.2322, -0.4574])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# 内置初始化\n","# m就是一个Module，如果Module是线性的/全连接层\n","def init_normal(m):\n","    if type(m) == nn.Linear:\n","        # normal_ 下划线在后面表示是一个替换函数，不会返回值\n","        nn.init.normal_(m.weight, mean=0, std=0.01)\n","        nn.init.zeros_(m.bias)\n","\n","# apply函数意思是对所以net里面的那些layer，一个一个做for loop\n","# 对net里所有module调用init_normal这个函数，把module做为参数传入\n","net.apply(init_normal)\n","net[0].weight.data[0], net[0].bias.data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVlkAOQo6YCj","executionInfo":{"status":"ok","timestamp":1712902041590,"user_tz":420,"elapsed":248,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"39a3fab6-5124-4c33-f2e5-3fc580d9fbb3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([-0.0139,  0.0003, -0.0014, -0.0150]), tensor(0.))"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# 初始成constant\n","def init_constant(m):\n","    if type(m) == nn.Linear:\n","        # weight初始为constant 1\n","        nn.init.constant_(m.weight, 1)\n","        nn.init.zeros_(m.bias)\n","\n","net.apply(init_constant)\n","net[0].weight.data[0], net[0].bias.data[0]\n","# api角度可以做，但是算法角度不能把weight全部初始化成常数"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-mbS_1w6ynB","executionInfo":{"status":"ok","timestamp":1712902123807,"user_tz":420,"elapsed":173,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"5f625df8-bafb-43a9-8179-45c4dcf3f052"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 1., 1., 1.]), tensor(0.))"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# 对某些块应用不同的初始化方法\n","# 对不同层apply不一样的东西\n","# xavier函数，用uniform distribution做初始化\n","def init_xavier(m):\n","    if type(m) == nn.Linear:\n","        nn.init.xavier_uniform_(m.weight)\n","\n","def init_42(m):\n","    if type(m) == nn.Linear:\n","        nn.init.constant_(m.weight, 42)\n","\n","# 第一个线性全连接层用Xavier来初始化\n","net[0].apply(init_xavier)\n","# 最后一个全连接层用init_42初始化\n","net[2].apply(init_42)\n","print(net[0].weight.data[0])\n","print(net[2].weight.data)\n","\n","# 因为每个层都是一个module，所以可以对任何一个层调用单独的初始化函数"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DI40MwBT7Gud","executionInfo":{"status":"ok","timestamp":1712902252011,"user_tz":420,"elapsed":167,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"2637f071-974e-4b29-9053-b9359cff926b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.5273,  0.2613,  0.2235, -0.0814])\n","tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"]}]},{"cell_type":"code","source":["# 自定义初始化\n","def my_init(m):\n","    if type(m) == nn.Linear:\n","        print(\"Init\",\n","              *[(name, param.shape) for name, param in m.named_parameters()][0])\n","        nn.init.uniform_(m.weight, -10, 10)\n","        # 保留绝对值大于5的权重，不是的就设为零\n","        m.weight.data *= m.weight.data.abs() >= 5\n","\n","net.apply(my_init)\n","net[0].weight[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyoukBoJ6X_r","executionInfo":{"status":"ok","timestamp":1712903520186,"user_tz":420,"elapsed":147,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"82d78350-ee04-4ca9-bf04-0ef50526f559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Init weight torch.Size([8, 4])\n","Init weight torch.Size([1, 8])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0000, 9.4622, -0.0000, 0.0000],\n","        [0.0000, -0.0000, 0.0000, -0.0000]], grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# 简单暴力的方法\n","# 所以值加1，第一个值等于42\n","net[0].weight.data[:] += 1\n","net[0].weight.data[0, 0] = 42\n","net[0].weight.data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e81DZ1Dx6X8h","executionInfo":{"status":"ok","timestamp":1712903538248,"user_tz":420,"elapsed":226,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"d752079b-9f35-4e29-8bac-47cedbbd6fcc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([42.0000, 10.4622,  1.0000,  1.0000])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# 参数绑定\n","# 我们想要在一些层里面share parameter -- 参数绑定\n","# 我们需要给共享层一个名称，以便可以引用它的参数\n","# 第二个和第三个隐藏层是shared权重，第一个和最后一个是自己的\n","# 理论上来说不管怎么更新这个network，他的第二个和第三个层都是一样的\n","shared = nn.Linear(8, 8)\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n","                    nn.ReLU(), nn.Linear(8, 1))\n","net(X)\n","# 检查参数是否相同\n","# 记得第3是relu，所以第二个共享层就是第4\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","net[2].weight.data[0, 0] = 100\n","# 改了第一个shared的weight的话，第二个shared的weight也是更改了，\n","# 因为它们指向了同样的类的实例，这就是如何在不同的网络之间共享权重的一个方法\n","# 确保它们实际上是同一个对象，而不只是有相同的值\n","print(net[2].weight.data[0] == net[4].weight.data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbnDMZcIAf6r","executionInfo":{"status":"ok","timestamp":1712904526058,"user_tz":420,"elapsed":179,"user":{"displayName":"Mingyuan Hua","userId":"11383044653926390732"}},"outputId":"12bc8529-127a-4a31-bde3-8fc0f2073227"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([True, True, True, True, True, True, True, True])\n","tensor([True, True, True, True, True, True, True, True])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"016ctV1JAf2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_CMBlE002usr"},"execution_count":null,"outputs":[]}]}